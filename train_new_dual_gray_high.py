# coding: utf-8
from __future__ import print_function
from __future__ import division
from data_parser.parse_tfrec import *
from loss.cal_loss import *
from utils.utilities import *
from utils.configs import *
import net.net_new_structure as ns
import time, os, sys
import imageio as io
import matplotlib.pyplot as plt

"""#####################################################################################################################
This file contains main to run the training.  Prior to run this the network, the data set should be generated by running
'data_process.gen_tfrec' file, and it will be placed under 'dataset' folder.

The network uses tfrecord and tensorflow Dataset api to feed data.

#####################################################################################################################"""

level = 'high'
lev_scale = '3'
batchnum = config.train.batchnum_high

tf.logging.set_verbosity(tf.logging.INFO)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"


def main(lev, goal_epoch):
    model_ckp = config.model.ckp_path_high + config.model.ckp_lev_scale + lev + '/gray/'
    tfrecord_path = '/media/ict419/SSD/SICE/' + 'high_freq' + '_' + lev + '_' + config.model.tfrecord_suffix

    with tf.device('/device:GPU:0'):
        with tf.Graph().as_default():
            with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:

                loss, inputimg, output, gt = trainlayer(tfrecord_path, sess)

                summary = tf.summary.merge_all()
                writer = tf.summary.FileWriter(model_ckp, sess.graph)

                global_step = tf.Variable(0, name="global_step", trainable=False)

                variable_to_train = []
                for variable in tf.trainable_variables():
                    if not (variable.name.startswith(config.model.loss_model)):
                        variable_to_train.append(variable)
                train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step=global_step,
                                                                 var_list=variable_to_train)

                variables_to_restore = []
                for v in tf.global_variables():
                    if not (v.name.startswith(config.model.loss_model)):
                        variables_to_restore.append(v)
                saver = tf.train.Saver(variables_to_restore, write_version=tf.train.SaverDef.V2)
                sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

                # restore variables for training model if the checkpoint file exists.
                epoch = restoreandgetepochs(model_ckp, sess, batchnum, saver)

                ####################
                """Start Training"""
                ####################
                start_time = time.time()
                while True:
                    _, loss_t, step, original, predict, gtruth = sess.run([train_op, loss, global_step, inputimg, output, gt])
                    batch_id = int(step % batchnum)
                    elapsed_time = time.time() - start_time
                    start_time = time.time()

                    """logging"""
                    tf.logging.info("Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.6f, global step: %4d"
                                    % (epoch + 1, batch_id, batchnum, elapsed_time, loss_t, step))

                    # advance counters
                    if batch_id == batchnum - 1:
                        if epoch >= goal_epoch:
                            break
                        else:
                            """checkpoint"""
                            saver.save(sess, os.path.join(model_ckp, 'pynets-model-high.ckpt'), global_step=step)
                        epoch += 1

                    """summary"""
                    if step % 15 == 0:
                        tf.logging.info('adding summary...')
                        summary_str = sess.run(summary)
                        writer.add_summary(summary_str, step)
                        writer.flush()

                        """
                        path = '/home/ict419/PycharmProjects/AVSSlapnet/evaluation/result/lev_scale_4/high_inter/'
                        result = 0
                        for i in range(config.train.batch_size_high):
                            result = predict[i,:,:,:]
                            io.imwrite(path + str(step) + '_' + str(i) + '_predict.jpg', result)
                            ori = original[i, :, :, :]
                            io.imwrite(path + str(step) + '_' + str(i) + '_input.jpg', ori)
                            label = gtruth[i, :, :, :]
                            io.imwrite(path + str(step) + '_' + str(i) + '_gt.jpg', label)

                            '''
                            plt.figure(0)
                            plt.subplot(221)
                            plt.imshow(norm_0_to_1(ori))
                            plt.subplot(222)
                            plt.imshow(norm_0_to_1(result))
                            plt.subplot(223)
                            plt.imshow(norm_0_to_1(label))
                            plt.show()
                            '''
                        """


def trainlayer(tfrecord_path, sess):
    ###################
    """Read tfrecord"""
    ###################
    train_iter = data_iterator_new_gray_high(tfrecord_path)
    img, gt = train_iter.get_next()

    '''######## data augmentation #######'''
    # random [0,1] float number. geo_trans[0] for flipping left & right, geo_trans[1] for flipping up & down
    # geo_trans[2] for rotation
    geo_trans = tf.random_uniform([3], 0, 1.0, dtype=tf.float32)

    # flip left & right
    img_lr = tf.cond(tf.less(geo_trans[0], 0.5), lambda: tf.image.flip_left_right(img), lambda: img)
    gt_lr = tf.cond(tf.less(geo_trans[0], 0.5), lambda: tf.image.flip_left_right(gt), lambda: gt)

    # flip up & down
    img_ud = tf.cond(tf.less(geo_trans[1], 0.5), lambda: tf.image.flip_up_down(img_lr), lambda: img_lr)
    gt_ud = tf.cond(tf.less(geo_trans[1], 0.5), lambda: tf.image.flip_up_down(gt_lr), lambda: gt_lr)

    # rotation
    d = tf.cast(geo_trans[2]*4+0.5, tf.int32)
    img_ready = tf.image.rot90(img_ud, d)
    gt_ready = tf.image.rot90(gt_ud, d)

    ##################
    """Feed Network"""
    ##################

    output = ns.nethighlayer_gray(img_ready)

    ##################
    """Build Losses"""
    ##################

    loss_l2_reg = 0
    loss_l1 = tf.reduce_mean(tf.abs(output - gt_ready))

    # Calculate L2 Regularization value based on trainable weights in the network:
    for variable in tf.trainable_variables():
        if not (variable.name.startswith(config.model.loss_model)):
            loss_l2_reg += tf.reduce_mean(tf.abs(variable)) * 2
        loss_l2_reg = tf.sqrt(loss_l2_reg)

    '''perceptual loss'''
    # duplicate the colour channel to be 3 same layers.
    output_3_channels = tf.concat([output, output, output], axis=3)
    gt_gray_3_channels = tf.concat([gt_ready, gt_ready, gt_ready], axis=3)

    losses = cal_loss(output_3_channels, gt_gray_3_channels, config.model.loss_vgg, sess)

    # perceptual loss
    loss_f = losses.loss_f / 3
    loss = loss_f * 0.5 + loss_l1 + loss_l2_reg

    #################
    """Add Summary"""
    #################
    tf.summary.scalar('loss/loss_l1', loss_l1)
    tf.summary.scalar('loss/loss_l2_reg', loss_l2_reg)
    tf.summary.scalar('loss/loss_f', loss_f * 0.5)
    tf.summary.scalar('loss/total_loss', loss)
    tf.summary.image('input', img_ready, max_outputs=12)
    tf.summary.image('output', output, max_outputs=12)
    tf.summary.image('ground_truth', gt_ready, max_outputs=12)

    """
    ''' restore high frequency vars '''
    variables_to_restore = []
    for v in tf.trainable_variables():
        if v.name.startswith('high'):
            variables_to_restore.append(v)

    model_ckp = config.model.ckp_path_high + config.model.ckp_lev_scale + lev_scale + '/'
    saver_h = tf.train.Saver(variables_to_restore, write_version=tf.train.SaverDef.V2)
    ckpt = tf.train.get_checkpoint_state(model_ckp)
    if ckpt and ckpt.model_checkpoint_path:
        full_path = tf.train.latest_checkpoint(model_ckp)
        saver_h.restore(sess, full_path)
    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])

    aa, bb, cc, dd = sess.run([img_ready, output, gt_ready, output - gt_ready])
    aa = np.squeeze(aa)
    bb = np.squeeze(bb)
    cc = np.squeeze(cc)
    dd = np.squeeze(dd)

    for i in range(config.train.batch_size_ft):
        plt.figure(0)
        plt.subplot(221)
        plt.imshow(aa[i, :, :], cmap='gray')
        plt.subplot(222)
        plt.imshow(bb[i, :, :], cmap='gray')
        plt.subplot(223)
        plt.imshow(cc[i, :, :], cmap='gray')
        plt.subplot(224)
        plt.imshow(dd[i, :, :], cmap='gray')
        plt.show()
    """
    return loss, img_ready, output, gt_ready


def load(ckpt_dir, sess, saver):
    tf.logging.info('reading checkpoint')
    ckpt = tf.train.get_checkpoint_state(ckpt_dir)
    if ckpt and ckpt.model_checkpoint_path:
        full_path = tf.train.latest_checkpoint(ckpt_dir)
        global_step = int(full_path.split('/')[-1].split('-')[-1])
        saver.restore(sess, full_path)
        return True, global_step
    else:
        return False, 0


def restoreandgetepochs(ckpt_dir, sess, batchnum, savaer):
    status, global_step = load(ckpt_dir, sess, savaer)
    if status:
        start_epoch = global_step // batchnum
        tf.logging.info('model restore success')
    else:
        start_epoch = 0
        tf.logging.info("[*] Not find pretrained model!")
    return start_epoch


main(lev_scale, 6)

